Part a: 
import numpy as np
import pandas as pd

def compute_p_ji_matrix(X, sigma):
    N, D = X.shape
    P = np.zeros((N, N))
    for i in range(N):
        dists_sq = np.sum((X[i] - X)**2, axis=1)
        dists_sq[i] = np.inf
        numerators = np.exp(-dists_sq / (2 * sigma**2))
        P[i, :] = numerators / np.sum(numerators)
    return P

Part b: 
def compute_p_ij_matrix(p_ji_matrix):
    N = p_ji_matrix.shape[0]
    return (p_ji_matrix + p_ji_matrix.T) / (2 * N)

Part c: 
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount("/content/drive", force_remount=True)
project_path = "/content/drive/My Drive/CS 121/Final"
%cd "/content/drive/My Drive/CS 121/Final"
!pwd
!ls

X = pd.read_csv("problem1.tsv", sep="\t", header=None).values

def plot_probabilities(X, sigma2):
    sigma = np.sqrt(sigma2)
    P = compute_p_ji_matrix(X, sigma)
    p1j = P[0, :]
    w = p1j / np.max(p1j)
    plt.figure(figsize=(6, 6))
    plt.scatter(X[:, 0], X[:, 1], c=w, cmap="Reds", s=30, edgecolor="k")
    plt.scatter(X[0, 0], X[0, 1], c="blue", s=60, label="Point 1", edgecolor="k")
    plt.title(f"$\\sigma^2$ = {sigma2}")
    plt.xlabel("x[0]")
    plt.ylabel("x[1]")
    plt.colorbar(label="Relative $p_{{1j}}$")
    plt.legend()
    plt.show()

sigmas2 = [0.1, 1, 10, 100]
for s2 in sigmas2:
    plot_probabilities(X, s2)

Part d: 
def compute_q_matrix(Y):
    N = Y.shape[0]
    q_matrix = np.zeros((N, N))
    dists_sq = np.sum((Y[:, np.newaxis, :] - Y[np.newaxis, :, :])**2, axis=2)
    inv_terms = 1 / (1 + dists_sq)
    np.fill_diagonal(inv_terms, 0)
    q_matrix = inv_terms / np.sum(inv_terms)

    return q_matrix

Part e:
def plot_q1j_probabilities(Y):
    Q = compute_q_matrix(Y)
    q1j = Q[0, :]
    weights = q1j / np.max(q1j)
    plt.figure(figsize=(6, 6))
    plt.scatter(Y[:, 0], Y[:, 1], c=weights, cmap="Blues", s=30, edgecolor="k")
    plt.scatter(Y[0, 0], Y[0, 1], c="red", s=60, edgecolor="k", label="Point 1")
    plt.title("$q_{1j}$ Probabilities with $y_i = x_i$")
    plt.xlabel("x[0]")
    plt.ylabel("x[1]")
    plt.colorbar(label="Relative $q_{1j}$")
    plt.legend()
    plt.show()

plot_q1j_probabilities(X)

Part f: 
def kl_divergence(P, Q, eps=0.000000000001):
    mask = P > 0
    kl = np.sum(P[mask] * (np.log(P[mask] + eps) - np.log(Q[mask] + eps)))
    return kl

Part g: 
sigma2_values = [0.1, 1, 100]
kl_results = {}

for sigma2 in sigma2_values:
    sigma = np.sqrt(sigma2)
    p_ji = compute_p_ji_matrix(X, sigma)
    P = compute_p_ij_matrix(p_ji)
    Q = compute_q_matrix(X)
    kl = kl_divergence(P, Q)
    kl_results[sigma2] = kl

print("KL Divergence:")
for s2, kl in kl_results.items():
    print(f"σ² = {s2}: KL = {kl:.4f}")

The KL-divergence is lowest when σ² = 1 (KL = 0.1054), which means the similarity structure in the original data is best matched by the low dimensional space. When σ² = 0.1, the KL value increases to 1.2980 because the similarity scores focus too much on close points, which the low dimensional space can’t represent easily. At σ² = 100, the KL drops to 0.7474 (lower than 0.1 but still not as good as 1). This suggests that σ² values that are too small or too large distort the relationships.

Part h: 

The value of σ² is important because it controls how we measure which points are close in the original space. If σ² is too small, we care about the closest points and ignore the rest, which can make the results less accurate. If it’s too large, everything will look equally close and we may lose some of the data’s structure. When σ² is well balanced, the low dimension map does the best job of matching the original data. So, choosing the right σ² value helps show the true shape of the data.

Part i: 
def apply_transformation(X, kind):
    Y = X.copy()
    if kind == "shift":
        Y[100:] -= 1.5
    elif kind == "scale_x":
        Y[:, 0] *= 0.7
    elif kind == "scale_y":
        Y[:, 1] *= 1.3
    elif kind == "rotate":
        angle = np.radians(90)
        R = np.array([
            [np.cos(angle), -np.sin(angle)],
            [np.sin(angle),  np.cos(angle)]
        ])
    elif kind == "rotate":
        angle = np.radians(90)
        R = np.array([
            [np.cos(angle), -np.sin(angle)],
            [np.sin(angle),  np.cos(angle)]
        ])
        Y_rot = []
        for point in Y:
            rotated_point = R.T @ point
            Y_rot.append(rotated_point)
        Y = np.array(Y_rot)
        Y[100:] -= 1.5
        Y[:, 0] *= 0.8
    return Y

def kl_divergence(P, Q, eps=0.000000000001):
    mask = P > 0
    return np.sum(P[mask] * (np.log(P[mask] + eps) - np.log(Q[mask] + eps)))

sigma = 1.0
p_ji = compute_p_ji_matrix(X, sigma)
P = compute_p_ij_matrix(p_ji)

Y_rotated = apply_transformation(X, "rotate")
Q_rotated = compute_q_matrix(Y_rotated)
kl_rotated = kl_divergence(P, Q_rotated)


import matplotlib.pyplot as plt
plt.figure(figsize=(6, 6))
plt.scatter(Y_rotated[:100, 0], Y_rotated[:100, 1], c="red", label="Cluster 1")
plt.scatter(Y_rotated[100:, 0], Y_rotated[100:, 1], c="blue", label="Cluster 2")
plt.title(f"Transformation: Rotate (KL = {kl_rotated:.4f})")
plt.xlabel("x[0]")
plt.ylabel("x[1]")
plt.legend()
plt.grid(True)
plt.show()

Part j: 
def compute_q_matrix(Y):
    N = Y.shape[0]
    dists_sq = np.sum((Y[:, np.newaxis, :] - Y[np.newaxis, :, :]) ** 2, axis=2)
    inv_terms = 1 / (1 + dists_sq)
    np.fill_diagonal(inv_terms, 0)
    return inv_terms / np.sum(inv_terms)

def kl_divergence(P, Q, eps=0.000000000001):
    mask = P > 0
    return np.sum(P[mask] * (np.log(P[mask] + eps) - np.log(Q[mask] + eps)))

def binary_search_sigma(X, target_perplexity, tol=0.0001, max_iter=50):
    N = X.shape[0]
    sigma2_vals = np.zeros(N)
    for i in range(N):
        beta_min, beta_max = 0.00001, 1000.0
        beta = 1.0
        for _ in range(max_iter):
            dists = np.sum((X[i] - X)**2, axis=1)
            dists[i] = np.inf
            P = np.exp(-beta * dists)
            P[i] = 0
            P /= np.sum(P)
            entropy = -np.sum(P[P > 0] * np.log2(P[P > 0]))
            perplexity = 2 ** entropy
            if abs(perplexity - target_perplexity) < tol:
                break
            elif perplexity > target_perplexity:
                beta_min = beta
                if beta_max < 1e5:
                    beta = (beta + beta_max) / 2
                else:
                    beta = beta * 2
            else:
                beta_max = beta
                if beta_min > 0.00001:
                    beta = (beta + beta_min) / 2
                else:
                    beta = beta / 2
        sigma2_vals[i] = 1 / (2 * beta)
    return sigma2_vals

def compute_p_ji_matrix_variable_sigma(X, sigma2_i):
    N = X.shape[0]
    P = np.zeros((N, N))
    for i in range(N):
        dists_sq = np.sum((X[i] - X) ** 2, axis=1)
        dists_sq[i] = np.inf
        numerators = np.exp(-dists_sq / (2 * sigma2_i[i]))
        P[i, :] = numerators / np.sum(numerators)
    return P

def compute_p_ij_matrix(p_ji):
    N = p_ji.shape[0]
    return (p_ji + p_ji.T) / (2 * N)

perplexities = [5, 25, 50, 100]
Q = compute_q_matrix(X)

results = {}

for perp in perplexities:
    sigma2_i = binary_search_sigma(X, perp)
    results[perp] = sigma2_i
    p_ji = compute_p_ji_matrix_variable_sigma(X, sigma2_i)
    P = compute_p_ij_matrix(p_ji)
    kl = kl_divergence(P, Q)
    print(f"Perplexity {perp}: KL-divergence = {kl:.4f}")

import matplotlib.pyplot as plt

plt.figure(figsize=(12, 6))
for perp, sigma2_i in results.items():
    plt.hist(sigma2_i, bins=30, alpha=0.6, label=f"Perplexity {perp}")

plt.title("Histogram of $\\sigma_i^2$ Values for Each Perplexity")
plt.xlabel("$\\sigma_i^2$")
plt.ylabel("Frequency")
plt.legend(title="Perplexity")
plt.grid(True)
plt.show()
